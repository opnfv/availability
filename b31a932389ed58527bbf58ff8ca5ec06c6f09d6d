{
  "comments": [
    {
      "key": {
        "uuid": "1a014df3_9eba3909",
        "filename": "gap_analysis/gap_analysis.rst",
        "patchSetId": 3
      },
      "lineNbr": 0,
      "author": {
        "id": 244
      },
      "writtenOn": "2015-06-16T23:30:18Z",
      "side": 1,
      "message": "It would be easier to review if the lines were broken up at page width.",
      "revId": "b31a932389ed58527bbf58ff8ca5ec06c6f09d6d",
      "serverId": "bbac25d2-bf60-4904-9ba8-a72fc000d6c5",
      "unresolved": false
    },
    {
      "key": {
        "uuid": "1a014df3_fe4a5d26",
        "filename": "gap_analysis/gap_analysis.rst",
        "patchSetId": 3
      },
      "lineNbr": 21,
      "author": {
        "id": 244
      },
      "writtenOn": "2015-06-16T23:30:18Z",
      "side": 1,
      "message": "I\u0027m not sure if I understood the problem. Is the problem that  one scheduler having the resource cannot release it because it doesn\u0027t get a chance to communicate? As a result other schedulers don\u0027t get the resource. This is why load balancing on the messaging would help.\n\nIf this is the case and I understood it correctly then I wouldn\u0027t call it race condition, it\u0027s more like starvation.",
      "range": {
        "startLine": 21,
        "startChar": 6,
        "endLine": 21,
        "endChar": 300
      },
      "revId": "b31a932389ed58527bbf58ff8ca5ec06c6f09d6d",
      "serverId": "bbac25d2-bf60-4904-9ba8-a72fc000d6c5",
      "unresolved": false
    },
    {
      "key": {
        "uuid": "1a014df3_de6e8196",
        "filename": "gap_analysis/gap_analysis.rst",
        "patchSetId": 3
      },
      "lineNbr": 41,
      "author": {
        "id": 244
      },
      "writtenOn": "2015-06-16T23:30:18Z",
      "side": 1,
      "message": "To me the problem seems to be that the compute service can die without notice. If we ensure that the compute service is running all the time (i.e. it\u0027s restarted in case of failure) then it should be able to report the health status.",
      "revId": "b31a932389ed58527bbf58ff8ca5ec06c6f09d6d",
      "serverId": "bbac25d2-bf60-4904-9ba8-a72fc000d6c5",
      "unresolved": false
    },
    {
      "key": {
        "uuid": "1a014df3_de47a11c",
        "filename": "gap_analysis/gap_analysis.rst",
        "patchSetId": 3
      },
      "lineNbr": 49,
      "author": {
        "id": 244
      },
      "writtenOn": "2015-06-16T23:30:18Z",
      "side": 1,
      "message": "This seems to be relevant too:\nhttps://review.openstack.org/#/c/138607/",
      "revId": "b31a932389ed58527bbf58ff8ca5ec06c6f09d6d",
      "serverId": "bbac25d2-bf60-4904-9ba8-a72fc000d6c5",
      "unresolved": false
    },
    {
      "key": {
        "uuid": "1a014df3_be6bb584",
        "filename": "gap_analysis/gap_analysis.rst",
        "patchSetId": 3
      },
      "lineNbr": 60,
      "author": {
        "id": 244
      },
      "writtenOn": "2015-06-16T23:30:18Z",
      "side": 1,
      "message": "Is the suggestion to have a nova API to \"track\" API the health status of some compute nodes? \nOr it\u0027s about the real-time update of the compute nodes health status in the nova database even?",
      "revId": "b31a932389ed58527bbf58ff8ca5ec06c6f09d6d",
      "serverId": "bbac25d2-bf60-4904-9ba8-a72fc000d6c5",
      "unresolved": false
    },
    {
      "key": {
        "uuid": "1a014df3_9e68f988",
        "filename": "gap_analysis/gap_analysis.rst",
        "patchSetId": 3
      },
      "lineNbr": 108,
      "author": {
        "id": 244
      },
      "writtenOn": "2015-06-16T23:30:18Z",
      "side": 1,
      "message": "It looks like the implementation had started and then it was abandoned. It\u0027d be good to know the reason.",
      "revId": "b31a932389ed58527bbf58ff8ca5ec06c6f09d6d",
      "serverId": "bbac25d2-bf60-4904-9ba8-a72fc000d6c5",
      "unresolved": false
    }
  ]
}